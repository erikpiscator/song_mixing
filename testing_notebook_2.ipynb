{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import csv\n",
    "import scipy\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Output structure:\n",
    "        list of dictionaries\n",
    "            playlist[n] = {\n",
    "                name (string)\n",
    "                audio_array (np.array)\n",
    "                sampling_rate (double)\n",
    "                ...\n",
    "                real_bpm (Int)\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Loading data from {path}...\")\n",
    "\n",
    "    playlist = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "\n",
    "            if file == \".DS_Store\":\n",
    "                continue\n",
    "\n",
    "            audio_file = AudioSegment.from_wav(os.path.join(root, file))\n",
    "\n",
    "            if audio_file.channels > 1:\n",
    "                # make sure we are only using one channel. It may not matter.\n",
    "                audio_file = audio_file.split_to_mono()[0]\n",
    "\n",
    "            audio_array = np.array(audio_file.get_array_of_samples(), dtype=float)\n",
    "            song_name, artist_name = extract_names(file)\n",
    "\n",
    "            song_dict = {\n",
    "                \"artist_name\": artist_name,\n",
    "                \"song_name\": song_name,\n",
    "                \"audio_segment\": audio_file,\n",
    "                \"audio_array\": audio_array,\n",
    "                \"song_path\": os.path.join(root, file),\n",
    "            }\n",
    "\n",
    "            playlist.append(song_dict)\n",
    "\n",
    "    playlist = basic_feature_extraction(playlist)\n",
    "    # playlist = load_true_bpm(playlist)\n",
    "\n",
    "    print(f\"\\t{len(playlist)} songs loaded\")\n",
    "\n",
    "    return playlist\n",
    "\n",
    "\n",
    "def extract_names(file):\n",
    "\n",
    "    song_name, _, artist_name = file.partition(\" - \")\n",
    "    song_name = song_name[3:]\n",
    "\n",
    "    artist_name, _, _ = artist_name.partition(\".\")\n",
    "\n",
    "    return song_name, artist_name\n",
    "\n",
    "\n",
    "def basic_feature_extraction(playlist):\n",
    "    \"\"\"\n",
    "    Output structure:\n",
    "        list of dictionaries\n",
    "            playlist[n] = {\n",
    "                name (string)\n",
    "                audio_array (np.array)\n",
    "                sampling_rate (double)\n",
    "                ...\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    for song in playlist:\n",
    "\n",
    "        song[\"frame_rate\"] = song[\"audio_segment\"].frame_rate\n",
    "\n",
    "    return playlist\n",
    "\n",
    "\n",
    "def load_true_bpm(playlist):\n",
    "\n",
    "    # load csv with the bpms\n",
    "\n",
    "    with open(\"songs.csv\", \"r\") as file:\n",
    "        csv_reader = csv.DictReader(file, delimiter=\",\")\n",
    "        playlist_true_bpm = list(csv_reader)\n",
    "\n",
    "    for song in playlist:\n",
    "        flag = 0\n",
    "        for song_ref in playlist_true_bpm:\n",
    "            if song[\"song_name\"] == song_ref[\"song_name\"]:\n",
    "                song[\"true_bpm\"] = song_ref[\"bpm\"]\n",
    "                flag = 1\n",
    "        if flag == 0:\n",
    "            # Don't know if this is the best way of raising an error.\n",
    "            # Please change to a better one if you know one.\n",
    "            print(\"No true bpm found for song:\", song[\"song_name\"])\n",
    "\n",
    "    return playlist\n",
    "\n",
    "\n",
    "def store_song(mix, path):\n",
    "\n",
    "    scipy.io.wavfile.write(\n",
    "        path, rate=mix[\"frame_rate\"], data=mix[\"audio_array\"].astype(\"int32\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant feature extraction\n",
    "# Beat detection\n",
    "# Key detection\n",
    "# Structural segmentation\n",
    "\n",
    "# from librosa.util.utils import frame\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from madmom.features.beats import RNNBeatProcessor\n",
    "from madmom.features.beats import DBNBeatTrackingProcessor\n",
    "from madmom.features.key import CNNKeyRecognitionProcessor\n",
    "from madmom.features.key import key_prediction_to_label\n",
    "\n",
    "import librosa\n",
    "\n",
    "import essentia\n",
    "from essentia.standard import FrameGenerator, PeakDetection\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "def feature_extraction(playlist):\n",
    "    print('Extracting features')\n",
    "\n",
    "    for i, song in enumerate(playlist):\n",
    "        print(f'\\tSong {i+1} / {len(playlist)}')\n",
    "\n",
    "        print('\\t\\tEstimating beat...')\n",
    "        beats_frames, bpm = beat_detection(song)\n",
    "        song['beat_times'] = beats_frames   # Array like the samples marking with the beat ocurrs, ones/zeros\n",
    "        song['estimated_bpm'] = bpm         # Int\n",
    "\n",
    "        print('\\t\\tEstimating key...')\n",
    "        key_probabilities, key_label = key_detection(song)\n",
    "        song['estimated_key'] = key_label.split(' ')[0]        # Probalby string or a int encoding of all the keys\n",
    "        song['estimated_mode'] = key_label.split(' ')[1]\n",
    "        song['key_probabilities'] = key_probabilities\n",
    "\n",
    "        print('\\t\\tEstimating cue-points')\n",
    "        cue_points = structural_segmentation(song)\n",
    "        song['cue_points'] = cue_points      # Array like the samples marking with the cue-point ocurrs \n",
    "\n",
    "        # Maybe cut silences or if the cue-points in\n",
    "        # the beginning and the end are too extreme\n",
    "\n",
    "    return playlist\n",
    "\n",
    "\n",
    "# FEATURES\n",
    "\n",
    "def beat_detection(song):\n",
    "\n",
    "    proc = DBNBeatTrackingProcessor(fps=100)\n",
    "    act = RNNBeatProcessor()(song[\"song_path\"])\n",
    "    beat_times = proc(act)\n",
    "\n",
    "    # create the array of ones and zeros\n",
    "    beat_frames = convert_to_frames(beat_times,song)\n",
    "\n",
    "    # compute the bpm of the song\n",
    "    bpm = beats_per_minute(beat_times,song)\n",
    "\n",
    "    return beat_frames, bpm\n",
    "\n",
    "\n",
    "def convert_to_frames(beat_times, song):\n",
    "\n",
    "    beat_frames = (beat_times*song[\"frame_rate\"]).astype(int)\n",
    "    beat_frames_mapped = np.zeros_like(song[\"audio_array\"])\n",
    "    beat_frames_mapped[beat_frames] = 1\n",
    "    \n",
    "    return beat_frames_mapped\n",
    "\n",
    "def beats_per_minute(beat_times, song):\n",
    "    \n",
    "    song_length = len(song[\"audio_array\"])/song[\"frame_rate\"]/60\n",
    "    beats_count = len(beat_times)\n",
    "    \n",
    "    bpm = beats_count/song_length # We could have problems with the first and the last beat\n",
    "    \n",
    "    return bpm\n",
    "\n",
    "\n",
    "def key_detection(song):\n",
    "\n",
    "    #key = rubberband/madmom (experiment with both)\n",
    "    \n",
    "    proc = CNNKeyRecognitionProcessor()\n",
    "    key_probabilities = proc(song[\"song_path\"])\n",
    "    key_label = key_prediction_to_label(key_probabilities)\n",
    "\n",
    "    return key_probabilities, key_label\n",
    "\n",
    "\n",
    "def structural_segmentation(song):\n",
    "\n",
    "    kernel_dim = 32\n",
    "    \n",
    "    samples_per_beat = int(1.0/(song['estimated_bpm']/(60.0 * song['frame_rate'])))\n",
    "\n",
    "    frame_size = int(0.5 * samples_per_beat)\n",
    "    hop_size = int(0.25 * samples_per_beat)\n",
    "\n",
    "    mfcc_ssm = mfcc_structural_similarity_matrix(song, frame_size=frame_size, hop_size=hop_size)\n",
    "    rms_ssm = rms_structural_similarity_matrix(song, frame_size=frame_size, hop_size=hop_size)\n",
    "\n",
    "    kernel = get_checkboard_kernel(kernel_dim)\n",
    "    mfcc_novelty = apply_kernel(mfcc_ssm, kernel)\n",
    "    rms_novelty = apply_kernel(rms_ssm, kernel)\n",
    "\n",
    "    size_dif = mfcc_novelty.size - rms_novelty.size\n",
    "    if size_dif > 0:\n",
    "        rms_novelty = np.pad(rms_novelty, (0, np.abs(size_dif)), mode='edge')\n",
    "    else:\n",
    "        mfcc_novelty = np.pad(mfcc_novelty, (0, np.abs(size_dif)), mode='edge')\n",
    "\n",
    "    novelty = mfcc_novelty * rms_novelty\n",
    "\n",
    "    peaks_rel_pos, peaks_amp = detect_peaks(novelty)\n",
    "    \"\"\"\n",
    "    save_cmap(mfcc_ssm, 'figures/mfcc_smm.png', ' MFCC Self-Similarity Matrix')\n",
    "    save_cmap(rms_ssm, 'figures/mfcc_smm.png', ' MFCC Self-Similarity Matrix')\n",
    "    save_cmap(kernel, 'figures/kernel', 'Checkboard Gaussian Kernel')\n",
    "    save_line(range(len(novelty)), novelty, 'figures/novelty.png', 'Novelty function', 'Frames', 'Amplitude')\n",
    "    save_line(peaks_rel_pos, peaks_amp, 'figures/peaks.png', 'Novelty peaks', 'Frames', 'Amplitude', '.')\n",
    "    \"\"\"\n",
    "    peaks_abs_pos = peaks_rel_pos * hop_size\n",
    "\n",
    "    peak_times = np.zeros_like(song['audio_array'])\n",
    "\n",
    "    for i in range(len(peaks_abs_pos)):\n",
    "        beat_peak = find_near_beat(peaks_abs_pos[i], song['beat_times'])\n",
    "        peak_times[beat_peak] = 1\n",
    "\n",
    "    return peak_times\n",
    "\n",
    "\n",
    "\n",
    "def mfcc_structural_similarity_matrix(song, frame_size, hop_size):\n",
    "\n",
    "    mspec = librosa.feature.melspectrogram(song['audio_array'], sr=song['frame_rate'], n_mels=128, n_fft=frame_size, window=\"hann\", win_length=frame_size, hop_length=hop_size,)\n",
    "\n",
    "    log_mspec = librosa.power_to_db(mspec, ref=np.max)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(S = log_mspec, sr=song['frame_rate'], n_mfcc=13)\n",
    "\n",
    "    ssm = sklearn.metrics.pairwise.cosine_similarity(mfcc.T, mfcc.T)\n",
    "    \n",
    "    ssm -= np.average(ssm)\n",
    "    m = np.min(ssm)\n",
    "    M = np.max(ssm)\n",
    "    ssm -= m\n",
    "    ssm /= np.abs(m) + M\n",
    "\n",
    "    return ssm\n",
    "\n",
    "\n",
    "def rms_structural_similarity_matrix(song, frame_size, hop_size):\n",
    "\n",
    "    rms_list = []\n",
    "    for frame in FrameGenerator(essentia.array(song['audio_array']), frameSize = frame_size, hopSize = hop_size):\n",
    "        rms_list.append(np.average(frame**2))\n",
    "\n",
    "    ssm = sklearn.metrics.pairwise.pairwise_distances(np.array(rms_list).reshape(-1, 1))\n",
    "\n",
    "    ssm -= np.average(ssm)\n",
    "    m = np.min(ssm)\n",
    "    M = np.max(ssm)\n",
    "    ssm -= m\n",
    "    ssm /= np.abs(m) + M\n",
    "\n",
    "    return ssm\n",
    "\n",
    "\n",
    "def get_checkboard_kernel(dim):\n",
    "\n",
    "    gaussian_x = scipy.signal.gaussian(2*dim, std = dim/2.0).reshape((-1,1))\n",
    "    gaussian_y = scipy.signal.gaussian(2*dim, std = dim/2.0).reshape((1,-1))\n",
    "\n",
    "    kernel = np.dot(gaussian_x,gaussian_y)\n",
    "\n",
    "    kernel[:dim,dim:] *= -1\n",
    "    kernel[dim:,:dim] *= -1\n",
    "\n",
    "    return kernel\n",
    "    \n",
    "\n",
    "def apply_kernel(ssm, kernel):\n",
    "\n",
    "    kernel_dim = int(kernel.shape[0]/2)\n",
    "    ssm_dim = ssm.shape[0]\n",
    "\n",
    "    novelty = np.zeros(ssm_dim)\n",
    "\n",
    "    ssm_padded = np.pad(ssm, kernel_dim, mode='edge')\n",
    "\n",
    "    for index in range(ssm_dim):\n",
    "        frame = ssm_padded[index:index+2*kernel_dim, index:index+2*kernel_dim]\n",
    "        novelty[index] = np.sum(frame * kernel)\n",
    "    \n",
    "    novelty /= np.max(novelty)\n",
    "\n",
    "    return novelty\n",
    "\n",
    "\n",
    "def detect_peaks(novelty):\n",
    "\n",
    "    threshold = np.max(novelty) * 0.025\n",
    "    \n",
    "    peakDetection = PeakDetection(interpolate=False, maxPeaks=100, orderBy='amplitude', range=len(novelty), maxPosition=len(novelty), threshold=threshold)\n",
    "    peaks_pos, peaks_ampl = peakDetection(novelty.astype('single'))\n",
    "    peaks_ampl = peaks_ampl[np.argsort(peaks_pos)]\n",
    "    peaks_pos = peaks_pos[np.argsort(peaks_pos)]\n",
    "\n",
    "    return peaks_pos, peaks_ampl\n",
    "\n",
    "\n",
    "def find_near_beat(position, beat_times):\n",
    "\n",
    "    position = int(position)\n",
    "\n",
    "    i_low = 0\n",
    "    i_up = 0\n",
    "    while(position - i_low > 0 and beat_times[position-i_low] == 0):\n",
    "        i_low += 1\n",
    "    while(position + i_up < len(beat_times) and beat_times[position+i_up] == 0):\n",
    "        i_up += 1\n",
    "\n",
    "    if i_low < i_up:\n",
    "        return position - i_low\n",
    "    else:\n",
    "        return position + i_up\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(playlist):\n",
    "\n",
    "    for song in playlist:\n",
    "        # Evaluating sort of acc in bpm detection\n",
    "        pass\n",
    "\n",
    "    # print or store or whatever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the first song\n",
    "# either:\n",
    "# iteratively choosing next song\n",
    "# tree search for optimal sequence\n",
    "\n",
    "\n",
    "circle_of_fifths = {\n",
    "    \"major\": [\"C\", \"G\", \"D\", \"A\", \"E\", \"B\", \"F#\", \"Db\", \"Ab\", \"Eb\", \"Bb\", \"F\"],\n",
    "    \"minor\": [\"A\", \"E\", \"B\", \"F#\", \"C#\", \"G#\", \"D#\", \"Bb\", \"F\", \"C\", \"G\", \"D\"],\n",
    "}\n",
    "scale = [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"F#\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "\n",
    "def get_song_sequence(playlist):\n",
    "\n",
    "    print(\"Selecting tracks order...\")\n",
    "\n",
    "    not_in_queue = playlist.copy()\n",
    "\n",
    "    not_in_queue.sort(key=lambda song: song[\"estimated_bpm\"])\n",
    "\n",
    "    queue = []\n",
    "\n",
    "    queue.append(not_in_queue.pop(0))\n",
    "\n",
    "    while not_in_queue:\n",
    "\n",
    "        next_song = pick_next_song(queue[-1], not_in_queue)\n",
    "        queue.append(next_song)\n",
    "        not_in_queue.remove(next_song)\n",
    "\n",
    "    return queue\n",
    "\n",
    "\n",
    "def pick_next_song(current, options):\n",
    "    \"\"\"\n",
    "    Explore several strategies\n",
    "\n",
    "    Example:\n",
    "        - Selecting candidate inside a +- bpm bounds\n",
    "        - Picking the most similar one in key\n",
    "        (see the paper for inspiration in distances between keys)\n",
    "    \"\"\"\n",
    "\n",
    "    threshold = 4\n",
    "\n",
    "    selection = None\n",
    "    current_bpm = current[\"estimated_bpm\"]\n",
    "    current_key_distance = 12  # Maximum distance\n",
    "\n",
    "    while not selection:\n",
    "\n",
    "        for song in options:\n",
    "\n",
    "            if (\n",
    "                song[\"estimated_bpm\"] >= current_bpm - threshold\n",
    "                and song[\"estimated_bpm\"] <= current_bpm + threshold\n",
    "            ):\n",
    "\n",
    "                optional_key_distance = key_distance_fifths(\n",
    "                    current[\"estimated_key\"],\n",
    "                    current[\"estimated_mode\"],\n",
    "                    song[\"estimated_key\"],\n",
    "                    song[\"estimated_mode\"],\n",
    "                )\n",
    "\n",
    "                if optional_key_distance < current_key_distance:\n",
    "\n",
    "                    selection = song\n",
    "                    current_key_distance = optional_key_distance\n",
    "\n",
    "        threshold += 2\n",
    "\n",
    "    return selection\n",
    "\n",
    "\n",
    "def key_distance_semitones(key1, key2):\n",
    "\n",
    "    idx1 = scale.index(key1)\n",
    "    idx2 = scale.index(key2)\n",
    "\n",
    "    diff = abs(idx1 - idx2)\n",
    "\n",
    "    distance = min(diff, 12 - diff)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def key_distance_fifths(key1, mode1, key2, mode2):\n",
    "\n",
    "    idx1 = circle_of_fifths[mode1].index(key1)\n",
    "    idx2 = circle_of_fifths[mode2].index(key2)\n",
    "\n",
    "    diff = abs(idx1 - idx2)\n",
    "\n",
    "    distance = min(diff, 12 - diff)\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively:\n",
    "# Create the transition for one pair of songs\n",
    "#   - Time wrapping (progressivelly better)\n",
    "#   - Key changing (explore strategies)\n",
    "#   - Align all the sequence according to modified beats (try to do it with downbeats)\n",
    "#   - Volume fades to mix both\n",
    "import numpy as np\n",
    "import rubberband as rb\n",
    "\n",
    "\n",
    "def create_transitions(queue):\n",
    "\n",
    "    mix = queue[0]\n",
    "\n",
    "    print(\"Creating_transitions...\")\n",
    "\n",
    "    for i in range(1, len(queue)):\n",
    "\n",
    "        print(f\"\\tMixing tracks {i} and {i+1}...\")\n",
    "        mix = mix_pair(mix, queue[i])\n",
    "\n",
    "    return mix\n",
    "\n",
    "\n",
    "def mix_pair(previous_mix, next_song):\n",
    "    \"\"\"\n",
    "    output\n",
    "        mix = {\n",
    "        name ([string])\n",
    "        audio_array (np.array)\n",
    "        sampling_rate ([double])\n",
    "        ...\n",
    "        real_bpm ([Int])\n",
    "        estimated_bpm ([Int])\n",
    "        estimated_key ([String])\n",
    "        cue_points (np.array)\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # selecting the actual cue-points from all the posibilities\n",
    "    previous_mix_cue_point = select_cue_points(previous_mix)\n",
    "\n",
    "    print(\"\\t\\tAligning songs...\")\n",
    "    next_song_aligned = align(next_song)\n",
    "\n",
    "    print(\"\\t\\tMixing beats...\")\n",
    "    previous_mix_stretched,next_song_stretched,previous_ending,next_beginning = time_wrap(previous_mix, next_song_aligned, previous_mix_cue_point)\n",
    "\n",
    "    #print(\"\\t\\tTransposing keys...\")\n",
    "    # previous_mix, next_song = key_change(previous_mix_stretched, next_song_stretched)\n",
    "\n",
    "    print(\"\\t\\tFading transition...\")\n",
    "    previous_mix_faded, next_song_faded = fade(previous_mix_stretched, next_song_stretched, previous_ending, next_beginning)\n",
    "\n",
    "    print(\"\\t\\tCombining tracks...\")\n",
    "    mix = combine_songs(previous_mix_faded, next_song_faded, previous_ending)\n",
    "\n",
    "    return mix #, previous_mix_faded, next_song_faded\n",
    "\n",
    "\n",
    "def select_cue_points(previous_mix):\n",
    "\n",
    "    max_transition_length = 120\n",
    "\n",
    "    cue_point = np.zeros_like(previous_mix[\"cue_points\"])\n",
    "\n",
    "    possible_idx = np.where(previous_mix[\"cue_points\"] == 1)[0]\n",
    "\n",
    "    # cue_point_idx = possible_idx[\n",
    "    #    possible_idx\n",
    "    #    > previous_mix[\"cue_points\"].size\n",
    "    #    - max_transition_length * previous_mix[\"frame_rate\"]\n",
    "    # ]\n",
    "    # print(cue_point_idx)\n",
    "\n",
    "    # if len(cue_point_idx) > 1:\n",
    "    #    cue_point_idx = cue_point_idx[1]\n",
    "    # else:\n",
    "    #    cue_point_idx = cue_point_idx[0]\n",
    "    flag = False\n",
    "    i = 1\n",
    "    while flag == False:\n",
    "        # select first cue point that are at least 20s from end.\n",
    "        if (len(previous_mix[\"audio_array\"]) - possible_idx[-i]) / previous_mix[\"frame_rate\"] >= 20:\n",
    "            cue_point[possible_idx[-i]] = 1\n",
    "            flag = True\n",
    "        i += 1\n",
    "    #cue_point_idx = possible_idx[-2]\n",
    "    \n",
    "\n",
    "    return cue_point\n",
    "\n",
    "\n",
    "def align(next_song):\n",
    "\n",
    "    first_beat = np.where(next_song[\"beat_times\"] == 1)[0][0]\n",
    "\n",
    "    new_next = next_song.copy()\n",
    "\n",
    "    new_next[\"audio_array\"] = next_song[\"audio_array\"][first_beat:]\n",
    "    new_next[\"beat_times\"] = next_song[\"beat_times\"][first_beat:]\n",
    "    new_next[\"cue_points\"] = next_song[\"cue_points\"][first_beat:]\n",
    "\n",
    "    return new_next\n",
    "\n",
    "\n",
    "def time_wrap(previous_mix, next_song, previous_mix_cue_point):\n",
    "\n",
    "    avg_bpm = (previous_mix[\"estimated_bpm\"] + next_song[\"estimated_bpm\"]) / 2\n",
    "\n",
    "    ending_stretching_ratio = previous_mix[\"estimated_bpm\"] / avg_bpm\n",
    "    beginning_stretching_ratio = next_song[\"estimated_bpm\"] / avg_bpm\n",
    "\n",
    "    cue_point_idx = np.where(previous_mix_cue_point == 1)[0][0]\n",
    "\n",
    "    #NEW-------------\n",
    "    \n",
    "    transition_length_seconds = 20\n",
    "\n",
    "    transition_length_prev_frames_stretched = transition_length_seconds * previous_mix[\"frame_rate\"]\n",
    "    transition_length_prev_frames = int(transition_length_prev_frames_stretched / ending_stretching_ratio)\n",
    "\n",
    "    transition_length_next_frames_stretched = transition_length_seconds * next_song[\"frame_rate\"]\n",
    "    transition_length_next_frames = int(transition_length_next_frames_stretched / beginning_stretching_ratio)\n",
    "    \"\"\"\n",
    "    print('beg len samp: ', transition_length_next_frames)\n",
    "    print('end len samp: ', transition_length_prev_frames)\n",
    "\n",
    "    print('beg bpm', previous_mix[\"estimated_bpm\"])\n",
    "    print('end bpm', next_song[\"estimated_bpm\"])\n",
    "\n",
    "    print('beg stretch', beginning_stretching_ratio)\n",
    "    print('end stretch', ending_stretching_ratio)\n",
    "    \"\"\"\n",
    "    ending_audio = previous_mix[\"audio_array\"][cue_point_idx : cue_point_idx + transition_length_prev_frames]\n",
    "    ending_beats = previous_mix[\"beat_times\"][cue_point_idx : cue_point_idx + transition_length_prev_frames]\n",
    "    beginning_audio = next_song[\"audio_array\"][:transition_length_next_frames]\n",
    "    beginning_beats = next_song[\"beat_times\"][:transition_length_next_frames]\n",
    "\n",
    "    \"\"\"\n",
    "    # ending_length_samples = previous_mix[\"audio_array\"].size - cue_point_idx\n",
    "    ending_length_samples = 20 * previous_mix[\"frame_rate\"]\n",
    "    transition_length = ending_length_samples * ending_stretching_ratio\n",
    "    transition_length_seconds = transition_length / previous_mix[\"frame_rate\"]\n",
    "    # if transition_length_seconds > 20:\n",
    "    #    transition_length_seconds = 20\n",
    "    \n",
    "    print(transition_length_seconds)\n",
    "\n",
    "    beginning_length_stretched = transition_length_seconds * next_song[\"frame_rate\"]\n",
    "    beginning_length_samples = int(beginning_length_stretched * beginning_stretching_ratio)\n",
    "\n",
    "    print('beg len samp: ', beginning_length_samples)\n",
    "    print('end len samp: ', ending_length_samples)\n",
    "\n",
    "    ending_audio = previous_mix[\"audio_array\"][cue_point_idx : cue_point_idx + ending_length_samples]\n",
    "    ending_beats = previous_mix[\"beat_times\"][cue_point_idx : cue_point_idx + ending_length_samples]\n",
    "    beginning_audio = next_song[\"audio_array\"][:beginning_length_samples]\n",
    "    beginning_beats = next_song[\"beat_times\"][:beginning_length_samples]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ending_audio_stretched = rb.stretch(np.array(ending_audio, dtype=\"int32\"),rate=previous_mix[\"frame_rate\"],ratio=ending_stretching_ratio,crispness=6,formants=False,precise=True)\n",
    "    beginning_audio_stretched = rb.stretch(np.array(beginning_audio, dtype=\"int32\"),rate=next_song[\"frame_rate\"],ratio=beginning_stretching_ratio,crispness=6,formants=False,precise=True)\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"end: \", len(ending_audio_stretched))\n",
    "    print(\"start: \", len(beginning_audio_stretched))\n",
    "    \"\"\"\n",
    "    ending_beats_stretched = stretch_beats(ending_beats, ending_stretching_ratio, ending_audio_stretched.size)\n",
    "    beginning_beats_stretched = stretch_beats(beginning_beats, beginning_stretching_ratio, beginning_audio_stretched.size)\n",
    "\n",
    "    previous_mix[\"estimated_bpm\"] = next_song[\"estimated_bpm\"]\n",
    "\n",
    "    new_previous = previous_mix.copy()\n",
    "    #new_previous[\"audio_array\"] = np.concatenate((new_previous[\"audio_array\"][:-ending_length_samples], ending_audio_stretched))\n",
    "    #new_previous[\"beat_times\"] = np.concatenate((new_previous[\"beat_times\"][:-ending_length_samples], ending_beats_stretched))\n",
    "    #new_previous[\"cue_points\"] = np.concatenate((new_previous[\"cue_points\"][:-ending_length_samples],np.zeros(ending_audio_stretched.size, dtype=previous_mix[\"cue_points\"].dtype)))\n",
    "\n",
    "    \n",
    "    new_previous[\"audio_array\"] = np.concatenate((new_previous[\"audio_array\"][:cue_point_idx], ending_audio_stretched))\n",
    "    new_previous[\"beat_times\"] = np.concatenate((new_previous[\"beat_times\"][:cue_point_idx], ending_beats_stretched))\n",
    "    new_previous[\"cue_points\"] = np.concatenate((new_previous[\"cue_points\"][:cue_point_idx],np.zeros(ending_audio_stretched.size, dtype=previous_mix[\"cue_points\"].dtype)))\n",
    "\n",
    "    new_next = next_song.copy()\n",
    "    new_next[\"audio_array\"] = np.concatenate((beginning_audio_stretched, new_next[\"audio_array\"][transition_length_next_frames:]))\n",
    "    new_next[\"beat_times\"] = np.concatenate((beginning_beats_stretched, new_next[\"beat_times\"][transition_length_next_frames:]))\n",
    "    new_next[\"cue_points\"] = np.concatenate((np.zeros(beginning_audio_stretched.size, dtype=next_song[\"cue_points\"].dtype),next_song[\"cue_points\"][transition_length_next_frames:]))\n",
    "\n",
    "    #return (new_previous,new_next,new_previous[\"audio_array\"][:-ending_length_samples].size,beginning_audio_stretched.size)\n",
    "    return (new_previous,new_next,new_previous[\"audio_array\"][:cue_point_idx].size,beginning_audio_stretched.size)\n",
    "\n",
    "def stretch_beats(beat_times, stretching_ratio, desired_length):\n",
    "\n",
    "    new_beats = []\n",
    "\n",
    "    zero_sequence_length = 0\n",
    "    for i in beat_times:\n",
    "        if i == 0:\n",
    "            zero_sequence_length += 1\n",
    "        elif i == 1:\n",
    "            new_beats += [0] * int(zero_sequence_length * stretching_ratio)\n",
    "            new_beats += [1]\n",
    "            zero_sequence_length = 0\n",
    "\n",
    "    diff = desired_length - len(new_beats)\n",
    "    if diff > 0:\n",
    "        new_beats += [0] * diff\n",
    "\n",
    "    return np.array(new_beats, dtype=int)\n",
    "\n",
    "\n",
    "def key_change(previous_mix, next_song, previous_mix_cue_point, next_song_cue_point):\n",
    "\n",
    "    # rubberband\n",
    "\n",
    "    # Choose to change the key of next_song completely or only the transition part\n",
    "\n",
    "    return previous_mix, next_song\n",
    "\n",
    "\n",
    "def fade(previous_mix, next_song, previous_mix_cue_point, next_song_cue_point):\n",
    "\n",
    "    fade_seconds = 20\n",
    "    fade_frames = fade_seconds * previous_mix[\"frame_rate\"]\n",
    "\n",
    "    for i in range(fade_frames):\n",
    "\n",
    "        #exponential fade\n",
    "        #previous_mix[\"audio_array\"][-i] = previous_mix[\"audio_array\"][-i] * (1.1 - np.exp(2.398 * (1 - i / fade_frames)) * 0.1)\n",
    "        #next_song[\"audio_array\"][i] = next_song[\"audio_array\"][i] * (0.1 * np.exp(2.398 * i / fade_frames) - 0.1)\n",
    "\n",
    "        #linear fade\n",
    "        previous_mix[\"audio_array\"][-i] = previous_mix[\"audio_array\"][-i] * i/fade_frames\n",
    "        next_song[\"audio_array\"][i] = next_song[\"audio_array\"][i] * i/fade_frames\n",
    "\n",
    "    return previous_mix, next_song\n",
    "\n",
    "\n",
    "def combine_songs(previous_mix, next_song, previous_ending):\n",
    "\n",
    "    mix = previous_mix.copy()\n",
    "\n",
    "    next_audio_padded = np.pad(next_song[\"audio_array\"], (previous_ending, 0), constant_values=0)\n",
    "    next_beat_padded = np.pad(next_song[\"beat_times\"], (previous_ending, 0), constant_values=0)\n",
    "    next_cue_padded = np.pad(next_song[\"cue_points\"], (previous_ending, 0), constant_values=0)\n",
    "\n",
    "    mix[\"audio_array\"] = next_audio_padded\n",
    "    mix[\"beat_times\"] = next_beat_padded\n",
    "    mix[\"cue_points\"] = next_cue_padded\n",
    "\n",
    "    mix[\"audio_array\"][: previous_mix[\"audio_array\"].size] += previous_mix[\"audio_array\"]\n",
    "    mix[\"beat_times\"][: previous_mix[\"beat_times\"].size] += previous_mix[\"beat_times\"]\n",
    "    mix[\"cue_points\"][: previous_mix[\"cue_points\"].size] += previous_mix[\"cue_points\"]\n",
    "\n",
    "    return mix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_cmap(matrix, filename, title='', xlabel='', ylabel='', colorbar=False):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    c = ax.pcolormesh(matrix, shading='auto', cmap='magma')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(xlabel)\n",
    "    if colorbar:\n",
    "        fig.colorbar(c, ax=ax)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def save_line(x, y, filename, title='', xlabel='', ylabel='', style=''):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.plot(x, y, style)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"songs/dev_songs_pop2020s/\"\n",
    "store_path = \"songs/dev_songs_pop2020s_output/song_mix_new_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from songs/dev_songs_pop2020s/...\n",
      "\t2 songs loaded\n"
     ]
    }
   ],
   "source": [
    "playlist = load_data(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "\tSong 1 / 2\n",
      "\t\tEstimating beat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tEstimating key...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tEstimating cue-points\n",
      "\tSong 2 / 2\n",
      "\t\tEstimating beat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tEstimating key...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tEstimating cue-points\n"
     ]
    }
   ],
   "source": [
    "playlist_features = feature_extraction(playlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tracks order...\n",
      "97.9951841565256   Just Wanna Be Friends\n",
      "122.3857498889914   Go On Together\n"
     ]
    }
   ],
   "source": [
    "queue = get_song_sequence(playlist_features)\n",
    "for song in queue:\n",
    "    print(song['estimated_bpm'], ' ', song['song_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating_transitions...\n",
      "\tMixing tracks 1 and 2...\n",
      "\t\tAligning songs...\n",
      "\t\tMixing beats...\n",
      "\t\tTransposing keys...\n",
      "\t\tFading transition...\n",
      "\t\tCombining tracks...\n"
     ]
    }
   ],
   "source": [
    "mix = create_transitions(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_song(mix, store_path)\n",
    "#store_song(previous_mix_faded, \"songs/dev_songs_house_output/prev_mix_faded_linear.wav\")\n",
    "#store_song(next_song_faded, \"songs/dev_songs_house_output/new_song_faded_linear.wav\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
